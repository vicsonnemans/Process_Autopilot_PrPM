# Process Autopilot: Intelligent Decision-Making for Improved Process Outcomes

## Summary of the paper
Traditional process mining primarily focuses on descriptive, diagnostic, and predictive analytics but lacks mechanisms for prescriptive optimization of business processes. Recently, various approaches to prescriptive process monitoring have been proposed, which mostly focus on defining ad-hoc abstractions of the process states over event data. However, a systematic, theoretic approach to reduce the high-dimensional space of processes while maintaining sufficient information for decision-making is still lacking.

In this paper, we introduce Process Autopilot, an offline-to-online Reinforcement Learning (RL) framework that leverages state abstraction for autonomously learning optimal decision strategies that maximize future process outcomes in a non-deterministic and stochastic environment. Our approach involves the discovery of a high-dimensional Markov Decision Process (MDP) from an input event log. The framework introduces and benchmarks multiple state abstraction strategies for automatically reducing the MDP’s state space, aiming to enhance the generalization of offline RL to process conditions beyond the training data. The offline policies trained on the abstracted MDP are then transferred to an online RL agent to accelerate learning. Evaluations on industrial event logs, using our publicly available implementation, demonstrate the effectiveness of abstraction functions that preserve essential state transition information while maintaining sufficient flexibility for generalization. Our results show that Process Autopilot with appropriate abstraction uncovers superior decision-making strategies, leading to a significant uplift in process outcomes.

## File Overview
#### `command.ls`
Contains all the python commands to replicate the results in the paper. You can also customize the config of hyperparameters by yourself.
#### `data/`:
This folder is where the original event logs and abstracted data are stored. 
- `df_simulation_preprocessedv2.csv`: Simulation event log generated by simulated MDP.
- `df_<dataset>_<k>_clusters.csv`: Log with full k-means-based abstracted states (generated when running `process_autopilot_training.py`).
- `df_<dataset>_<k>_clusters_features.csv`: Log with partial k-means-based abstracted states (generated when running `process_autopilot_training.py`).
- `df_<dataset>_<k>_structural.csv`: Log with structural abstraction (generated when running `process_autopilot_training.py`).
- `df_<dataset>_preprocessedv2.csv`: Preprocessed real event logs (e.g., BPI2017).
#### `src/`:
This directory contains all the Python scripts used for simulation, training, and evaluation of the Process Autopilot framework with various state abstraction techniques.
- `simulated_offline_online_RL.py`  
  Trains **offline-to-online** Process Autopilot on simulated data with various state abstraction methods.

- `simulated_online_nopretraining.py`  
  Trains online RL from scratch, without offline pretraining.

- `process_autopilot_training.py`  
  Trains the offline Process Autopilot on **real event logs** (i.e., BPIC17 and BPIC12) with or without abstractions.

- `state_abstraction.py`  
  Performs **state abstraction** using: `k_means` (full k-means abstraction),`k_means_features` (partial k-means abstraction), `structural` abstraction.

- `MDP_generator.py`  
  This script defines a custom Gym-based reinforcement learning environment that simulates a loan application process as an MDP with business logic, stochastic customer behavior, and domain-specific constraints.
  
- `preprocessing.py`  
  Preprocesses real-world event logs: construct control-flow variables, remove redundant events, incomplete or infrequent traces, environmental actions, etc. 

- `MDP_functions.py`  
  Utility functions for constructing the MDP.

- `utils.py`  
  Utility functions for generating event logs, formatting states, etc.

- `bisimulation.ipynb`  
  Used to compute the intra-cluster bisimulation loss of each state abstraction technique. It evaluates how well abstracted states preserve the original transition and reward dynamics.

#### `results/`
Stores all training results and evaluation files.

## Simulated MDP

The repository contains a simulated MDP — an online Gym environment that models a simplified loan origination processs as a Markov Decision Process (MDP).

#### State Space

- **Static applicant attributes** (sampled once at case start):
  - `loan_amount` – \$1,000–\$40,000 (step \$5,000)
  - `credit_score` – 300–850 (step 100)
  - `debt_to_income` – 0–100% (step 30%)
  - `purpose` – 4 categorical loan goals
- **Dynamic case attributes** (updated at each step):
  - `num_calls`, `num_offers`, `offer_amount`
  - Environmental flags: `response_to_call`, `acceptance_of_offer`, `cancellation`
- **`last_action`** – the most recent activity index
- **Observation**: 4 static + 6 dynamic + 1 action = 11-dimensional integer vector

#### Action Space

9 discrete actions represent workflow activities:

```
0 receive_application       3 create_offer          6 approve  
1 request_docs              4 schedule_call         7 reject  
2 escalate                  5 flag_manual           8 cancellation_received  
```

#### Transition Logic

- *Allowed actions* depend on both the previous activity and business constraints:
  - Cannot approve without offer acceptance
  - Escalate is disallowed for high-credit applicants
  - Calls and offers have upper bounds
- *Stochastic customer behavior* generates non-deterministic and stochastic transitions:
  - 60% chance customer responds to calls  
  - 50% chance customer accepts an offer  
  - 5% chance of customer cancellation at any step

#### Reward Structure

- +10 for approval  
- −10 for rejection  
- −20 for cancellation  
- −1 for intermediate steps  
- −100 for invalid actions (immediate termination)

#### Episode Termination

The process ends when:
- The loan is **approved**, **rejected**, or **cancelled**

> The Python class is thoroughly commented – consult the source code for parameter ranges, stochastic settings, and constraint logic.